# Generatings-songs-GRU

In this notebook I trained a neural network to generate lyrics of a song based on a given melody - the melodies are stored in MIDI files.

## Data analysis:

The data contains lyrics of 600 songs in the training set, and 6 in the test set. 
In addition to that, it contains 628 MIDI files. 

## Data Processing and Feature Engineering:

**For the lyrics** - I started by tokenizing the lyrics of each song and removing special characters to obtain a clear representation of the words. Next, I created two dictionaries - one that mapped each word to an index and another that mapped the index back to the word. To represent the words using word embeddings, I used the pre-trained word2vec model with 300 dimensions.

During the training process, I cleaned the lyrics by following the above steps and looked up the corresponding word embedding representation for each word in the created dictionary. If a word was not in the dictionary, which was rare, I assigned it a vector of zeros with 300 dimensions.

**for the melodies** -  

I utilized two different techniques to extract melody features. The first approach involved loading chroma and piano roll matrices and then flattening them into a vector using the average values. This approach considered the time dimension in the song.

In the second approach, a matrix of size (128, 9) was created to represent each of the 128 instruments in nine features. These features included the number of notes, the number of pitch bends, the number of control changes, the maximum and minimum pitch of a note, the average pitch of the notes, the maximum speed of notes, the minimum speed of notes, and the average speed of all notes. Unlike the first approach, there was no reference to the time dimension in this method.

I combined the melody features with the lyrics and, in rare cases where MIDI files were not found, I created a vector filled with zeros.

## The model:

The model I used for this work employs GRU layers to process the input and linear layers to map the output from the GRU to the prediction of the next word in the sequence. 
o generate predictions, I applied softmax to convert logits into probabilities for each word.

To prevent overfitting, I added a dropout layer to the model - This layer randomly sets some input units to 0 during training,
which helps to prevent the model from becoming too dependent on specific input features and results in better generalization to new data.

![image](https://user-images.githubusercontent.com/96613758/220572581-382dc0a7-a26c-4c71-a540-18f72c2c4bd7.png)

## Training the model:

The model was trained using 20 epochs and the Adam optimizer. Adam optimizer is a widely used optimization algorithm for deep learning models, which adjusts the learning rate for each weight in the network, providing better convergence and faster training.

During training, the cross-entropy loss function was utilized to compute the model error. Cross-entropy is a common loss function for multi-class classification problems like this, which compares the predicted probabilities of each class to the actual target class and computes the distance between them.

To evaluate the model's performance, a validation set was created with an 80-20 split. The cross-entropy loss function was used to compute the model's error on this validation set. This helped to monitor the model's performance and ensure that it was not overfitting to the training data.

## Generating songs:

The generate_song function is responsible for creating a song given a trained model, a starting set of lyrics, and a MIDI file. The function uses the softmax function on the predicted logits of the model to sample the next word of the song. This word is then used as the input for the next prediction of the model. The function generates the song lyrics in a Python loop.

It's important to note that the words generated by the model in each step may not necessarily be chosen based on the highest probabilities, making the model non-deterministic. The maximum number of words in a song was limited to 100.

## Evaluation and results:

**first model**

![image](https://user-images.githubusercontent.com/96613758/220573966-a7271b06-14ec-4c98-b3e4-03c96ddca03d.png)

![image](https://user-images.githubusercontent.com/96613758/220574005-d1eb2f76-a153-4b79-9855-52125bc36a5d.png)

** second model**

![image](https://user-images.githubusercontent.com/96613758/220574092-d624f056-1442-4fff-9cfe-1868b22b2e85.png)

![image](https://user-images.githubusercontent.com/96613758/220574121-7e05fb7f-850c-49fe-92f4-ee6b0b147291.png)

**an example of a generated song:**

![image](https://user-images.githubusercontent.com/96613758/220574218-7fe594fa-fecd-48e0-afdd-5f2f8362245e.png)



